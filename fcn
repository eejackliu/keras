Dice loss 对于二分类效果明显，bceloss需要尝试，链接1,2是各种不同的loss。链接1中的get_border可得到比target物体本身border大一圈的范围，用来和y_label
求loss。在unet的bottleneck层应该使用有别与upsample过程中的放大参数，（20，15）下采样再上采样之后是（20，14）下采样过程使用same模式可能会引入噪声
对于（320，240）放缩五次之后就会面临上采样和原来大小不一的问题。目前看来采用较大的上采样值然后crop掉（transpose conv 特定，bilinear只能成倍数放大）
比事后pad（（0，0，1，0）这个是在垂直方向pad1个像素）要好一点点，需要证实。
loss的实现影响最大，因为背景作为others的预测不能主导loss的值，crossentrpy把背景算在loss中，这样可能会陷入局部最小（全部预测为背景也是一种解）
keras版本的mobilenet是valid，midd层会少1个像素，padding0比padding reflect效果差5% tf.keras 和 keras 只能二选一，tflie-from-keras用keras
的model能跑通
nout= (nin+2p-k)/s +1 这个是计算conv输出面积的，同时也是计算这个维度中元素（feature）的个数的，在计算感受野（receptive field）的时候需要用到
jump这个概念，其实就是后一个卷及比前一个卷及平移几个元素，第一层这个值为1，下一层的jump=jump（本层的）＊stride（conv的stride）
下一层的rceptive field=r(本层 receptive）+(kernel size（本层）-1)*jump(本层） 注意 jump是累进的，vgg中主要是pooling累进jump的值
感受野特指这个面积大小的元素会最终影响结果中的一个元素
A guide to receptive field arithmetic for Convolutional Neural Networks这个论文里面也显示了尽管最后一层橘颜色的元素的感受野大小是7＊7，和
输入一样大，但是那一层仍然有四个元素，感受野类似kernel，输入太小还可以加padding（这里指的是等效padding，具体来源于每一层的conv的padding，因为在
计算receptive field的时候没考虑过进行convolution的元素输入来自哪里，只考虑了几个kernel叠加起来相当与几乘几的conv，所以这么算也只是等效kernel
实际的感受野没有那么大，除非把每一层的用于‘SAME’的padding取消，实际感受野才会和计算的一样，如果padding用reflect（不知道这样padding的元素和输出相关
这种假的感受野会不会有助于增加对图像的感知）
http://zike.io/posts/calculate-receptive-field-for-vgg-16/这个页面详细计算了累计receptive filed的计算式
deep lab 中的这句话 
 After the first fully connected layer has 4,096 fil-ters of large 7 × 7 spatial size and becomes the computational bottleneck in our dense score map
computation. 他是指把那7＊7＊512 个特征变成fc中的4096个参数需要 4096个7＊7那么大的连接网络
cnn的equivariant vs. invariant （https://zhuanlan.zhihu.com/p/41682204）
Rethinking Atrous Convolution for Semantic Image Segmentation 若想提高对于不同体积的识别率可以用不同大小的图片在inference时输入然后综合结果
在分类中resnet和mobilenet中pooling的个数不是很多，output_stride是input image spatial resoulution 和final output resolution的比率，感受
野是每个像素。这就是stride，是每个等效conv跳过的面积，就是小于这个值的物体会被忽略。相临两个像素的感受野有可能都很大（比如200＊200），但是很大一部分是
重叠的，增加的stride的才是新的不同的信息.连续的atrous使用相同的rate会产生gridding问题
understanding convolution for semantic segmentation 连续几层使用相同r的dilate conv会让第二层中的dilate conv 的kernel非0部分与之前的棋盘
叠加，造成局部信息缺失，获得过于large distance的信息。其中3.2 
“maximum distance between two nonzero values  as  Mi = max[Mi+1 − 2ri, Mi+1 − 2(Mi+1 − ri ), ri ], 这是指一系列的dilated rates
［r1,r2,r3....rn] 最后一层中r就是两个非0kernel元素的间隔（dilated的kernel），这个公式是要倒退回之前的一层，看看是不是因为dilated的叠加会使mi大于
kernel（大于kernel会造成获得信息的距离过远）这一系列的r不能有超过1的公约数。否则棋盘效果仍在. 3.2.1中的multi-grid说rates=2＊（1，2，4）这里的1，2，4
指的是dialte中的hole的数量，棋盘的产生也是因为连续的几个conv的hole有倍数关系。rates要在这个mutigrid上乘以2是因为这一层不用pool来扩大感受野
而用dialte conv扩大感受野，pool的倍数是2，这里也就是2.因为kernel中间插的是hole，hole扩大几倍，kernel就扩大几倍。同理，后面的bolock就是4，8依次类推
需要上采样的倍数为r，当前网络特征图的维度为 h × w × c ￼ ，分割任务共需要预测L类结果。则DUC模块仅需要在之前的特征后添加一个全连接层，输出h × w × (d2 × L), ￼ 的分割结果。
其中心点在于DUC将最后需要预测得到的高分辨率分割结果(尺寸为L＊rH*rW ￼ )拆分为更小、通道数更多的小分割结果。相当与resnet或者mobilenet中的channel数量
先膨胀再卷及然后缩小channel数量。意思就是说一个h＊w＊c的 matrix 可以被小分辨率但是更多的channel代替。这里的DUC就是反向的过程，把提取到的众多特征先用
1＊1的conv当全连接变成(d＊＊2 × L)个channel（2是平方的意思）然后用这个逆过程还原图片
Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation  pooling就是stride不为1的conv，多个layer叠加
stride得相乘，因为第二层stride过的元素每一个都是stride第一层的结果


论文state of art
fcn 全卷积，以及 最后的实验证明类别比例与照片比例不一致是不影响结果的
unet overlap-tile方式以及u形结构
TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation 使用参数vgg预训练参数，上采样部分要取下采样block在maxpool前的最后一个（还得是2的次方）
而且同一倍数的层中，上采样是下采样倍数的一半。这在人像上比unet那种镜像对称的效果好（nest形式中三层高5%，五层的高2%）
The Importance of Skip Connections in Biomedical Image Segmentation这里面提到他们使用dice loss会曾在边界缺失的问题，bce则会是模糊的状态
使用0.2dropout的dice loss效果要好
deeplab ：vgg中到后面几层随着jump越来越大，按照receptive file算，feature中每个点是一个很大receptive filed的结果，这些点数量很少，那么在从原图
提取feature数量这个角度来讲提取的过少，是sparse的，所以需要得到dense spatial score，也就是很深的层也得有很多feature，同时还得保证receptive file不能
比原来对应那层小。要dense spatial就得使后面的层size不能减小太多（可以减少pooling），要 recepctive就得有更大的kernel但又不能减少feature的数量（所以不能用pooling来
获得更大的receptive field）有不能真的增大很多kernel（因为stride小的kernel会和临近的kernel产生重叠，而导致增加receptive filed增加量有限，其实tux
金字塔也可以，当然这是deeplav3的），就诞生了带洞的conv 。其实就是因为vgg原本的7＊7那层是32的stride，这32个pixel对于最后的结果影响太小，所以结果sparse spatial score
deeplabv1 是把最后两层的pooling的stride2改成了1，这样就是下采样8倍，atrous conv用了padding之后就不会缩小feature的尺寸，用atrous中的hole的比例
来代替原本用maxpooling造成的receptive field增长。所以第4个pooling之后的conv的hole是2，第5个pooling之后的hole是4，hole的倍数就是kernel的倍数
label需要用双线性插值将lable降低至1／8，这样在真值周围会因为插值造成模糊（比真值小一点的数）。用这个去做crossentry会造成边缘模糊

model distillation ：Caruana and his collaborators  考虑到softmax后的lable会使一些概率小的量变成离0很近，不利于计算lossfunction（乘后容易变0）
使用进入softmax之前的logits来作为 softlabel 。small model和 softlabel之间的loss是squared difference（logitd的大模型和要训练的小模型）

need to do：

IDW-cnn 弱监督语义分割 deeplabv2 提到过
上采样中的conv可以尝试用mobilenet中的深度卷积和channel卷积代替
模型运行出的结果简单以0.5作为阈值来判断像素是否为背景，用val集来找阈值应该效果会更好
使用sequeezenet来构建，conv层有没有pading=1的影响。
unet论文中使用overlap-tile方式把大的图片（3000＊3000）切成小的照片（本质上是同样物体的重复出现）但是且出来的照片会有一圈padding，这些padding来自
与切的时候相邻的区域，没有则以镜像padding解决，pytorch中以replicate参数决定。值得尝试这种方案，如果不用这种方案而使用same的conv就能取得不错效果则不用
上采样过程中逐步以近似0.5倍率效果貌似比ternausnet中要好
DeepLabv3:Rethinking Atrous Convolution for Semantic Image Segmentation 这里在introduction的fig1 提到image pyramid来提取多种scale的feature
是否可以学习encorder-decorde模式，把中间几层当作image pyramid的一部分来和最后的一层组成image pyramid
需要阅读的论文
Focal Loss for Dense Object Detection一


Multi-class segmentation of neuronal
structures in electron microscopy images



keras ＆ torch
keras想要进行element wise的运算最好使用 lambda 例子：keras.layers.Lambda(lambda x:(1/temp)*x)(x)







import tensorflow as tf
sess=tf.Session()
# tf.saved_model.loader.load(sess,[tf.saved_model.tag_constants.SERVING],'deeplabv3_mnv2_dm05_pascal_trainval')
# with tf.Session() as sess:
f=open('./deeplabv3_mnv2_dm05_pascal_trainval/frozen_inference_graph.pb', 'rb')
graph_def = tf.GraphDef()
graph_def.ParseFromString(f.read())
# print (graph_def)
# tf.import_graph_def(graph_def)
graph_node=[ i for i in graph_def.node]
graph_name=[i.name for i in graph_node]
graph_value=[ None  if i.attr['value'].ByteSize()==0 else tf.make_ndarray(i.attr['value'].tensor) for i in graph_node]
c={i:j for i, j in zip(graph_name,graph_value)}
# with open('qwe','w') as f:
#     for i in graph_name:
#         f.writelines(i+'\n')
# import numpy as np
# np.save('ten.npy',c)
# l=[]
# for i in graph_node:
#     if  i.attr['value'].ByteSize()==0:
#         l.append(None)
#     else:
#         try:
#            l.append(tf.make_ndarray(i.attr['value']))
#         except:
#             l.append('error')
# r=dict()
# for i,j in zip(graph_node,graph_value):
#     try:
#         r[i]=j
#     except:
#         print(i)
#         print(j)




1：https://github.com/killthekitten/kaggle-carvana-2017/blob/master/losses.py
2：https://www.zhihu.com/question/264537057
